---
layout: post
title: Ext3 file fragmentation on my server
date: '2010-06-27T03:13:00.000-07:00'
author: Stephen Nancekivell
tags:
- Linux
- Server
modified_time: '2012-09-22T23:05:11.280-07:00'
blogger_id: tag:blogger.com,1999:blog-2519344180739427574.post-7200196692189090828
blogger_orig_url: http://blog.stephenn.com/2010/06/ext3-file-fragmentation-on-my-server.html
---

So&nbsp;I've&nbsp;got this file server/torrent box running raid 5 with 5 discs ATM, but i &nbsp;can add more disks to it when it gets full and have been doing so for the last year. Lately we've been noticing terrible performance from it, like browsing a folder (over samba) could take about 30 seconds to load.<br /><br />Looking into this i found that the files being downloaded slowly by the torrents are getting terribly&nbsp;fragmented, &nbsp;like, from filefrag<br /><blockquote><span style="font-size: 13px; line-height: 19px;">old file: 629 extents found, perfection would be 29 extents</span></blockquote><br /><blockquote><span style="font-size: 13px; line-height: 19px;"> </span><span style="font-size: 13px; line-height: 19px;">recently downloaded file: 56289 extents found, perfection would be 12 extents</span></blockquote><br /><span style="font-size: small; line-height: 19px;">So thats about 4.6k times worse than it should be.</span><br /><br />It seems theres been some&nbsp;negligence&nbsp;of this issue, people argue as long as you have some free space ext3 wont get fragmented. Thus there is no proper defragger for ext3. There was one for ext2, and ext3 is backward compatible so could use that, but i'd have to take the fileserver down for some time, and i like using it. There are some other small tools "Shake"&nbsp;and "defrag", but im going to make my own.<br /><br />I'm going to do some shitty defragging, by copying fragmented files to another place on the disk deleting the old copy and moving the new one in its place. The new file should be less fragmented as the files ystem will try to allocate space in a continuous block. There may be a good reason why I shouldn't&nbsp;do this but, i'm going to strive ahead and see what happens.<br /><br />Heres a simple test on one file. copying it, looking at its fragmentation.<br /><blockquote><span style="font-style: normal;"><span style="font-size: small; line-height: 27px;">84025 to 434</span><span style="font-size: small; line-height: 27px;"> : 1.5gig file, perfection would be 13.</span></span></blockquote><br />So its not making stuff perfect which is&nbsp;disappointing, i may end up doing multiple sweeps with this. But thats still 193 times better than it was.<br /><br /><span style="font-size: x-small; line-height: 19px;"><span style="font-size: medium;"><span style="line-height: 24px;">Second sweep:</span></span></span><br /><blockquote><span style="font-size: x-small; line-height: 19px;"><span style="font-size: medium;"><span style="line-height: 24px;">434 to 223: 1.5 gig file,&nbsp;perfection&nbsp;would be 13.</span></span></span></blockquote><br /><span style="font-size: medium;">Second sweep only gives a 2x improvement.</span><br /><br /><span style="font-size: medium;">Heres my script to do some defragmenting, its pretty over the top, with the md5sum checks. I've never seen the copy corrupt a file, but i'd rather be safe than sorry. By sorting the files by most fragmented, and keeping track of the improvement, i can stop if it starts making stuff worse. And by sorting, work on the worst files first.</span><br /><br /><pre class="prettyprint">#!/bin/bash<br /># a script to fight file fragmentation.<br /><br />DIR=$1<br />if [[ $DIR == "" ]]<br />then<br />echo "invalid usage"<br />exit 1<br />fi<br /><br />TMP='/tmp/fragment_1.tmp'<br />CDIR="$DIR/copied_for_fragmentation/"<br />mkdir "$CDIR"<br /><br />rm $TMP<br /># filefrag everything under directory.<br /><br />echo "calculating frags for files."<br /><br />find "$DIR" |<br />while read line<br />do<br />FGS=`filefrag "$line" | awk -F':' '{print $2}' | awk '{print $1}'`<br />echo $FGS::$line &gt;&gt; $TMP<br />done<br /><br /># sort by most fragmented<br />sort -nr $TMP &gt; "$TMP"2<br />mv "$TMP"2 $TMP<br /><br />cat "$TMP" |<br />while read line<br />do<br />FG1=`echo $line| awk -F'::' '{print $1}'`  # number of fragments file is in<br /># copy file to different place.<br /><br />C1=`echo $line| awk -F'::' '{print $2}'`<br />C1copy="$CDIR/c1.bin"<br /># echo "copying to new file $C1"<br />cp "$C1" "$C1copy"<br />if [ $? -ne 0 ]; then echo "error copying $C1, aborting"; exit 1; fi<br /><br /># check files have some contents<br /># echo "checking md5sums"<br />MD1=`md5sum "$C1" | awk '{print $1}'`<br />MD2=`md5sum "$C1copy" | awk '{print $1}'`<br /><br />if [ $MD1 != $MD2 ]; then echo "md5's dont match for $C1 "; rm "$C1copy"; exit 1; fi<br /><br />#check frags<br /># echo "checking frags"<br />FG2=`filefrag "$C1copy" | awk -F':' '{print $2}' | awk '{print $1}'` # number of fragments new file is in.<br />if (( $FG1 &lt; $FG2 ))<br />then<br />echo "copying made this file more fragmented!  $C1"<br />rm "$C1copy"<br />exit 1<br />fi<br /><br /># move new into olds place<br /># echo "moving $C1"<br />mv "$C1copy" "$C1"<br />if [ $? -ne 0 ]; then echo " error moving copy overtop of file"; exit 1; fi<br /><br /># permissions<br />chown root:shared "$C1" &amp;amp;amp;amp;&amp;amp;amp;amp; chmod a+rwx "$C1"<br />if [ $? -ne 0 ]; then echo "error setting permissions on $C1"; exit 1; fi<br /><br />echo "$FG1 to $FG2 : $C1"<br />done<br /><br />rmdir "$CDIR"<br />rm $TMP<br /></pre><br />That script changes all the file permissions &amp; mod/create times, while these aren't important for me, it would have been nice to handle those.