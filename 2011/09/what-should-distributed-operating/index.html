<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>What should a Distributed Operating System should look like and how close are we to making one | Stephen's Tech Blog</title><meta name=keywords content><meta name=description content="Overview - A Distributed Operating SystemA Distributed operating system is a system that operates a distributed collection of computers, a cluster. Some require a single controller, but that shouldn't be necessary and isn't an interesting part of the problem. The system runs programmes and manages resources. The system should completely abstract the view of the hardware away from the software.
Fault tolerance, scalability and redundancyThe cluster may be heterogeneous, nodes may go offline, new nodes may come online."><meta name=author content="Stephen Nancekivell"><link rel=canonical href=https://stephenn.com/2011/09/what-should-distributed-operating/><link crossorigin=anonymous href=/assets/css/stylesheet.min.d1b059b1e59019acbcb9abc899cab7a2eaa982555ca8891f1416094bb8854660.css integrity="sha256-0bBZseWQGay8uavImcq3ouqpglVcqIkfFBYJS7iFRmA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js integrity="sha256-uVus3DnjejMqn4g7Hni+Srwf3KK8HyZB9V4809q9TWE=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://stephenn.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://stephenn.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://stephenn.com/favicon-32x32.png><link rel=apple-touch-icon href=https://stephenn.com/apple-touch-icon.png><link rel=mask-icon href=https://stephenn.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=icon href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üëã&zwj;‚ôÇÔ∏è</text></svg>"><script async src="https://www.googletagmanager.com/gtag/js?id=G-LNCP0VXVCC"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-LNCP0VXVCC",{anonymize_ip:!1})}</script><meta property="og:title" content="What should a Distributed Operating System should look like and how close are we to making one"><meta property="og:description" content="Overview - A Distributed Operating SystemA Distributed operating system is a system that operates a distributed collection of computers, a cluster. Some require a single controller, but that shouldn't be necessary and isn't an interesting part of the problem. The system runs programmes and manages resources. The system should completely abstract the view of the hardware away from the software.
Fault tolerance, scalability and redundancyThe cluster may be heterogeneous, nodes may go offline, new nodes may come online."><meta property="og:type" content="article"><meta property="og:url" content="https://stephenn.com/2011/09/what-should-distributed-operating/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2011-09-19T06:58:00-07:00"><meta property="article:modified_time" content="2011-09-19T06:58:00-07:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="What should a Distributed Operating System should look like and how close are we to making one"><meta name=twitter:description content="Overview - A Distributed Operating SystemA Distributed operating system is a system that operates a distributed collection of computers, a cluster. Some require a single controller, but that shouldn't be necessary and isn't an interesting part of the problem. The system runs programmes and manages resources. The system should completely abstract the view of the hardware away from the software.
Fault tolerance, scalability and redundancyThe cluster may be heterogeneous, nodes may go offline, new nodes may come online."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://stephenn.com/posts/"},{"@type":"ListItem","position":3,"name":"What should a Distributed Operating System should look like and how close are we to making one","item":"https://stephenn.com/2011/09/what-should-distributed-operating/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"What should a Distributed Operating System should look like and how close are we to making one","name":"What should a Distributed Operating System should look like and how close are we to making one","description":"Overview - A Distributed Operating SystemA Distributed operating system is a system that operates a distributed collection of computers, a cluster. Some require a single controller, but that shouldn't be necessary and isn't an interesting part of the problem. The system runs programmes and manages resources. The system should completely abstract the view of the hardware away from the software.\nFault tolerance, scalability and redundancyThe cluster may be heterogeneous, nodes may go offline, new nodes may come online.","keywords":[],"articleBody":" Overview - A Distributed Operating SystemA Distributed operating system is a system that operates a distributed collection of computers, a cluster. Some require a single controller, but that shouldn't be necessary and isn't an interesting part of the problem. The system runs programmes and manages resources. The system should completely abstract the view of the hardware away from the software.\nFault tolerance, scalability and redundancyThe cluster may be heterogeneous, nodes may go offline, new nodes may come online. These are all issues that the system should handle. The system should dynamically adapt to changes in its configuration while continuing to function without error. At all times the system should utilize the cluster to its full potential. Nothing of value should be lost when a node does go down, thus redundancy needs to be built into the system.\nAbstractionFor a distributed operating system, abstraction of the whole computer cluster needs to be considered. The application programmer needs the ability to create threads, manage the communication between them and access resources. The actual distribution and synchronisation should to be managed by the system.\nIntro to componentsCompute power distributionDistributing computational tasks between computers can be difficult or easy depending on the type of task being performed.\nSome task fall into the embarrassingly parallel category. This is where there is lots of data items that needs the same processing done to each. Here the data can simply be partitioned and sent to different CPU‚Äôs to work on independently.\nOther tasks fall into the non-embarrassingly parallel category. These tasks require more complex coordination to work in parallel. Non-embarrassingly parallel tasks traditionally use shared variables and require locking of variables.\nIn both cases there are different threads of execution work for some process. These threads will be distributed over different CPU‚Äôs and computer nodes within the cluster.\nRequirementsThe distributed operating system needs to abstract the hardware and run the programs. While some overhead will be accepted the efficiency of the system is a priority. Hardware abstraction has been done very well in modern computers. There are programming languages like C that compile for many different machine architecture‚Äôs and languages like Java that run in a virtual machine environment.\nResource managementSome sort of shared storage is required. For different applications a traditional file system or a database might be required. These would use the storage of all the nodes, while maintaining redundancy. The file system and database should behave like traditional implementations.\nCompute power distributionI propose the process‚Äôs and threads get a context that abstracts and simplifies their distribution and fault tolerance. These context‚Äôs will be similar to the contexts of the thread/process on a single computer operating system, with fault tolerant and scalability extensions. Contexts would define inter-thread communication and be associated with shared variables and thread locks. A global Context would exist for the entire process. Smaller Context groups could be defined for sub groups of threads within the process. The partitioning of threads will help the system optimize the layout of the nodes and limit the scope of shared resources reducing redundant copies being updated.\nInter thread communication, shared variablesIt may be that a small group of threads need to work on a sub problem for a larger task. The parallelization would need shared variables. Normally this would require a lot of inter-thread communication, but through the use of these contexts that could be abstracted away. The threads would simply access the shared variable. The computer nodes that these threads work on should have minimal latency. By being in their own context group the system could know to put them close together. This is an example of a environment aware optimization.\nSometimes threads may need to wait for other threads. These waits could be managed by these thread context groups.\nFault toleranceThe thread contexts would ensure fault tolerance by state saving to some distributed memory. Then ensuring that the thread is still running with ‚Äòheart beat‚Äô messages. If the thread is found to have crashed another thread could be started from the last context save. Here the only thing of value lost is the computation time.\nIn traditional operating systems, threads are swapped out to allow multiple treads running on a single CPU. This is similar to the context saves in this distributed system. The same kind of information is being saved.\nThe context saves could be an expensive part of the distribution of tasks. Each save comes with a cost, but each crash comes with another cost lessened by the context save. Balancing the two costs could be quite difficult, especially for different cluster configurations. The distributed operating system should take care of how often context saves occur. By learning the cost of each save and how vulnerable the cluster is, a automated balancing algorithm could be implemented.\nLocksLocks should generally be avoided, especially in a redundant distributed context. A lot of locks could be able to avoided through the use of higher level data structures. For example In a producer consumer situation a thread safe queue maybe simpler to work with than a simple array. The array is a much simpler to implement, but isn‚Äôt thread safe so thread locking would be required, where the queue could be thread safe, requiring minimal locking from the clients.\nCompare and swap style locking operations would also work well in a distributed system.\nTraditional locks will may be required at some point, or may just be desired by application programmers, so they should be implemented. To ensure redundancy with traditional locks, the locks should have a TTL (time to live). Where if the TTL expires the lock invalidates leaving the rest of the program to continue. The thread with the lock could renew the TTL, to acquire more time with it. This could behave like heart beat messages to ensure fault tolerance.\nHeterogeneous competitionWith the context saves continually happening to the distributed system, two copies of the same thread context save could be started on different nodes. One may finish faster, the results from that node could be taken and the slower node could abandon its progress, as it would lead to the same result. This would be useful when the cluster is waiting for one thread to finish, which may be on a slower computer. This optimization was shown in the google map reduce paper [GMR] to greatly improve the overall performance of the cluster.\nEmbarrassingly parallelThe distribution of embarrassingly parallel tasks has been done well by the google map reduce [GMR] and the Apache hadoop project [AHD]. Similar map reduce functionality could easily be implemented within this cluster using these contexts. One thread could read the input and start worker threads which would send their results to reducer threads.\nResource managementFile SystemThe distribution of the file system can be achieved by placing only part of the file system on each particular node called a brick. For redundancy duplicates of each brick would be replicated on different computer nodes.\nThe client using the file system would query which node a particular block was on then get that block from the respective node. This would all be packaged in the file system driver and appear like a normal file system to the applications.\nThe file system needs to support growing, having more storage added to it. This can be achieved by using a dynamic file mapping structure. Which would need to be distributed and redundant across the cluster.\nHigher throughput could be achieved by striping the data across multiple servers. This is done by partitioning the data into blocks, where successive blocks go to different cluster nodes. This provides the speed when one block is physically being written or read from one disk, the next block can start being written / read from the next node. Other environmentally aware optimizations could also be implemented, such as file locality.\nShared memoryThe system should store its context saves in memory. These will be broadcast to other nodes for redundancy. Not all nodes need to store the contexts for all other threads, just enough for redundancy. This could require lots of memory. But the treads can choose when they have reached a place to save state, ensuring that they only save whats required. To ensure context saves don't come out of order, they will have a timestamp. So if two computers compute the same context state, but one is further through its computation than the other, each thread will know how far through it is. The node receiving both will know what one to keep, the one with the higher time stamp.\nThe system will need to use a similar communication and shared memory structure for coordinating the execution of thread contexts. Care will need to be taken to ensure race conditions don't occur. The scheduling and starting of threads could be done in a two step phase. Where in the first phase the thread to start is scheduled, communicated to all nodes, and any race conditions are dealt with. The second phase takes place when the schedule is agreed upon, and the thread is actually started.\nHow close we are to building one.Some of the components already exist, while others need work. Work needs to be done to package everything together for easy distribution.\nDistributed File systems already exist, there is the Google file system [GFS] and the Gluster file system [GLS].\nRedundant coordination already exists in some computer games. With multiple computers playing one network game, one computer is chosen as the server. If that server then goes offline the other computers re-elect a new server. This is similar to the coordination required for a distributed operating system.\nApache ZooKeeper also provides redundant distributed coordination software [AZK].\nWe have distributed hash tables and distributed databases like Apache Cassandra [ACD], which work like a distributed shared memory. However their usually for persistent storage not like fast volatile shared memory required for task coordination.\nRedundant distributable task contexts have been done well for embarrassingly parallel tasks, with map reduce and hadoop. Building on my proposed model should work well, but needs more work to show that its feasible, the context saving needs to be proven to not be too expensive, then it needs to be implemented.\nSo how close. A year with a solid development team should be enough to put something together, using another distributed file system and data base. The big problem will be getting it popular enough to gain momentum. A lot of the companies that could provide the initial push of development seem to work in more of a embarrassingly parallel world, so wouldn't benefit from this.\nReferences[GMR] Jeffrey Dean \u0026 Sanjay Ghemawat (2004) MapReduce: Simplied Data Processing on Large Clusters. Google, Inc\n[AHD] http://hadoop.apache.org\n[GFS] Sanjay Ghemawat, Howard Gobioff \u0026 Shun-Tak Leung (2003) The Google File System. Google, Inc\n[GLS] http://www.gluster.org/\n[AZK] http://zookeeper.apache.org/\n[ACD] http://cassandra.apache.org/","wordCount":"1802","inLanguage":"en","datePublished":"2011-09-19T06:58:00-07:00","dateModified":"2011-09-19T06:58:00-07:00","author":{"@type":"Person","name":"Stephen Nancekivell"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://stephenn.com/2011/09/what-should-distributed-operating/"},"publisher":{"@type":"Organization","name":"Stephen's Tech Blog","logo":{"@type":"ImageObject","url":"https://stephenn.com/favicon.ico"}}}</script></head><body id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://stephenn.com accesskey=h title="Stephen's Tech Blog (Alt + H)">Stephen's Tech Blog</a>
<span class=logo-switches></span></div><ul id=menu><li><a href=https://webhookwizard.com title="Webhook Wizard üßô‚Äç‚ôÇÔ∏è"><span>Webhook Wizard üßô‚Äç‚ôÇÔ∏è</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>What should a Distributed Operating System should look like and how close are we to making one</h1><div class=post-meta><span title='2011-09-19 06:58:00 -0700 -0700'>September 19, 2011</span>&nbsp;¬∑&nbsp;9 min&nbsp;¬∑&nbsp;Stephen Nancekivell</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#internal-source-marker_0.4227949823252857 aria-label="Overview - A Distributed Operating System"><span class=Apple-style-span style=font-family:Arial;font-size:24px;white-space:pre-wrap>Overview - A Distributed Operating System</span></a><ul><ul><ul><li><a href=# aria-label="
"><span style=background-color:transparent;color:#000;font-family:Arial;font-size:12pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap><br></span></a></li><li><a href=# aria-label="Fault tolerance, scalability and redundancy"><span style=background-color:transparent;color:#000;font-family:Arial;font-size:12pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Fault tolerance, scalability and redundancy</span></a></li></ul><li><a href=# aria-label=Abstraction><span style=background-color:transparent;color:#000;font-family:Arial;font-size:14pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Abstraction</span></a></li></ul><li><a href=# aria-label="Intro to components"><span style=background-color:transparent;color:#000;font-family:Arial;font-size:18pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Intro to components</span></a><ul><li><a href=# aria-label="Compute power distribution"><span style=background-color:transparent;color:#000;font-family:Arial;font-size:14pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Compute power distribution</span></a><ul><ul><li><a href=# aria-label=Requirements><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Requirements</span></a></li></ul></ul></li><li><a href=# aria-label="Resource management"><span style=background-color:transparent;color:#000;font-family:Arial;font-size:14pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Resource management</span></a></li></ul></li><li><a href=# aria-label="Compute power distribution"><span style=background-color:transparent;color:#000;font-family:Arial;font-size:18pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Compute power distribution</span></a><ul><ul><ul><li><a href=# aria-label="Inter thread communication, shared variables"><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Inter thread communication, shared variables</span></a></li><li><a href=# aria-label="Fault tolerance"><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Fault tolerance</span></a></li><li><a href=# aria-label=Locks><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Locks</span></a></li><li><a href=# aria-label="Heterogeneous competition"><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Heterogeneous competition</span></a></li><li><a href=# aria-label="Embarrassingly parallel"><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Embarrassingly parallel</span></a></li></ul></ul><li><a href=# aria-label="Resource management"><span style=background-color:transparent;color:#000;font-family:Arial;font-size:14pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Resource management</span></a><ul><ul><li><a href=# aria-label="File System"><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>File System</span></a></li><li><a href=# aria-label="Shared memory"><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Shared memory</span></a></li></ul></ul></li></ul></li><li><a href=# aria-label="How close we are to building one."><span style=background-color:transparent;color:#000;font-family:Arial;font-size:18pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>How close we are to building one.</span></a><ul><ul><ul><li><a href=# aria-label=References><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>References</span></a></li></ul></li></ul></ul></ul></li></ul></div></details></div><div class=post-content><br><div style=background-color:transparent><h1 dir=ltr id=internal-source-marker_0.4227949823252857 style=margin-bottom:0;margin-top:0;text-align:left><span class=Apple-style-span style=font-family:Arial;font-size:24px;white-space:pre-wrap>Overview - A Distributed Operating System</span></h1><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>A Distributed operating system is a system that operates a distributed collection of computers, a cluster. Some require a single controller, but that shouldn't be necessary and isn't an interesting part of the problem. The system runs programmes and manages resources. The system should completely abstract the view of the hardware away from the software.</span><br><h4 dir=ltr><span style=background-color:transparent;color:#000;font-family:Arial;font-size:12pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap><br></span></h4><h4 dir=ltr><span style=background-color:transparent;color:#000;font-family:Arial;font-size:12pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Fault tolerance, scalability and redundancy</span></h4><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>The cluster may be heterogeneous, nodes may go offline, new nodes may come online. These are all issues that the system should handle. The system should dynamically adapt to changes in its configuration while continuing to function without error. At all times the system should utilize the cluster to its full potential. Nothing of value should be lost when a node does go down, thus redundancy needs to be built into the system.</span><br><h3 dir=ltr><span style=background-color:transparent;color:#000;font-family:Arial;font-size:14pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Abstraction</span></h3><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>For a distributed operating system, abstraction of the whole computer cluster needs to be considered. The application programmer needs the ability to create threads, manage the communication between them and access resources. The actual distribution and synchronisation should to be managed by the system.</span><br><h2 dir=ltr><span style=background-color:transparent;color:#000;font-family:Arial;font-size:18pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Intro to components</span></h2><h3 dir=ltr><span style=background-color:transparent;color:#000;font-family:Arial;font-size:14pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Compute power distribution</span></h3><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Distributing computational tasks between computers can be difficult or easy depending on the type of task being performed.</span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap></span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Some task fall into the embarrassingly parallel category. This is where there is lots of data items that needs the same processing done to each. Here the data can simply be partitioned and sent to different CPU‚Äôs to work on independently.</span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap></span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Other tasks fall into the non-embarrassingly parallel category. These tasks require more complex coordination to work in parallel. Non-embarrassingly parallel tasks traditionally use shared variables and require locking of variables.</span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap></span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>In both cases there are different threads of execution work for some process. These threads will be distributed over different CPU‚Äôs and computer nodes within the cluster.</span><br><h5 dir=ltr><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Requirements</span></h5><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>The distributed operating system needs to abstract the hardware and run the programs. While some overhead will be accepted the efficiency of the system is a priority. Hardware abstraction has been done very well in modern computers. There are programming languages like C that compile for many different machine architecture‚Äôs and languages like Java that run in a virtual machine environment.</span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap></span><br><h3 dir=ltr><span style=background-color:transparent;color:#000;font-family:Arial;font-size:14pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Resource management</span></h3><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Some sort of shared storage is required. For different applications a traditional file system or a database might be required. These would use the storage of all the nodes, while maintaining redundancy. The file system and database should behave like traditional implementations.</span></div><div style=background-color:transparent><span style=background-color:transparent;color:#000;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline></span><span class=Apple-style-span style=font-family:Arial><span class=Apple-style-span style=font-size:15px;white-space:pre-wrap><br></span></span><h2 dir=ltr><span style=background-color:transparent;color:#000;font-family:Arial;font-size:18pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Compute power distribution</span></h2><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>I propose the process‚Äôs and threads get a context that abstracts and simplifies their distribution and fault tolerance. These context‚Äôs will be similar to the contexts of the thread/process on a single computer operating system, with fault tolerant and scalability extensions. Contexts would define inter-thread communication and &nbsp;be associated with shared variables and thread locks. A global Context would exist for the entire process. Smaller Context groups could be defined for sub groups of threads within the process. The partitioning of threads will help the system optimize the layout of the nodes and limit the scope of shared resources reducing redundant copies being updated.</span><br><h5 dir=ltr><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Inter thread communication, shared variables</span></h5><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>It may be that a small group of threads need to work on a sub problem for a larger task. The parallelization would need shared variables. Normally this would require a lot of inter-thread communication, but through the use of these contexts that could be abstracted away. The threads would simply access the shared variable. The computer nodes that these threads work on should have minimal latency. By being in their own context group the system could know to put them close together. This is an example of a environment aware optimization.</span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap></span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Sometimes threads may need to wait for other threads. These waits could be managed by these thread context groups.</span><br><h5 dir=ltr><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Fault tolerance</span></h5><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>The thread contexts would ensure fault tolerance by state saving to some distributed memory. Then ensuring that the thread is still running with ‚Äòheart beat‚Äô messages. If the thread is found to have crashed another thread could be started from the last context save. Here the only thing of value lost is the computation time.</span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap></span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>In traditional operating systems, threads are swapped out to allow multiple treads running on a single CPU. This is similar to the context saves in this distributed system. The same kind of information is being saved.</span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap></span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>The context saves could be an expensive part of the distribution of tasks. Each save comes with a cost, but each crash comes with another cost lessened by the context save. Balancing the two costs could be quite difficult, especially for different cluster configurations. The distributed operating system should take care of how often context saves occur. By learning the cost of each save and how vulnerable the cluster is, a automated balancing algorithm could be implemented.</span><br><h5 dir=ltr><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Locks</span></h5><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Locks should generally be avoided, especially in a redundant distributed context. A lot of locks could be able to avoided through the use of higher level data structures. For example In a producer consumer situation a thread safe queue maybe simpler to work with than a simple array. The array is a much simpler to implement, but isn‚Äôt thread safe so thread locking would be required, where the queue could be thread safe, requiring minimal locking from the clients.</span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap></span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Compare and swap style locking operations would also work well in a distributed system.</span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap></span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Traditional locks will may be required at some point, or may just be desired by application programmers, so they should be implemented. To ensure redundancy with traditional locks, the locks should have a TTL (time to live). Where if the TTL expires the lock invalidates leaving the rest of the program to continue. The thread with the lock could renew the TTL, to acquire more time with it. This could behave like heart beat messages to ensure fault tolerance.</span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap></span><br><h5 dir=ltr><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Heterogeneous competition</span></h5><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>With the context saves continually happening to the distributed system, two copies of the same thread context save could be started on different nodes. One may finish faster, the results from that node could be taken and the slower node could abandon its progress, as it would lead to the same result. This would be useful when the cluster is waiting for one thread to finish, which may be on a slower computer. This optimization was shown in the google map reduce paper [GMR] to greatly improve the overall performance of the cluster.</span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap></span><br><h5 dir=ltr><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Embarrassingly parallel</span></h5><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>The distribution of embarrassingly parallel tasks has been done well by the google map reduce [GMR] and the Apache hadoop project [AHD]. Similar map reduce functionality could easily be implemented within this cluster using these contexts. One thread could read the input and start worker threads which would send their results to reducer threads.</span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap></span><br><h3 dir=ltr><span style=background-color:transparent;color:#000;font-family:Arial;font-size:14pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Resource management</span></h3><h5 dir=ltr><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>File System</span></h5><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>The distribution of the file system can be achieved by placing only part of the file system on each particular node called a brick. For redundancy duplicates of each brick would be replicated on different computer nodes.</span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap></span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>The client using the file system would query which node a particular block was on then get that block from the respective node. This would all be packaged in the file system driver and appear like a normal file system to the applications.</span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap></span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>The file system needs to support growing, having more storage added to it. This can be achieved by using a dynamic file mapping structure. Which would need to be distributed and redundant across the cluster.</span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap></span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Higher throughput could be achieved by striping the data across multiple servers. This is done by partitioning the data into blocks, where successive blocks go to different cluster nodes. This provides the speed when one block is physically being written or read from one disk, the next block can start being written / read from the next node. Other environmentally aware optimizations could also be implemented, such as file locality.</span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap></span><br><h5 dir=ltr><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Shared memory</span></h5><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>The system should store its context saves in memory. These will be broadcast to other nodes for redundancy. Not all nodes need to store the contexts for all other threads, just enough for redundancy. This could require lots of memory. But the treads can choose when they have reached a place to save state, ensuring that they only save whats required. To ensure context saves don't come out of order, they will have a timestamp. So if two computers compute the same context state, but one is further through its computation than the other, each thread will know how far through it is. The node receiving both will know what one to keep, the one with the higher time stamp.</span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap></span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>The system will need to use a similar communication and shared memory structure for coordinating the execution of thread contexts. Care will need to be taken to ensure race conditions don't occur. The scheduling and starting of threads could be done in a two step phase. Where in the first phase the thread to start is scheduled, communicated to all nodes, and any race conditions are dealt with. The second phase takes place when the schedule is agreed upon, and the thread is actually started.</span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap></span><br><br><h2 dir=ltr><span style=background-color:transparent;color:#000;font-family:Arial;font-size:18pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>How close we are to building one.</span></h2><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Some of the components already exist, while others need work. Work needs to be done to package everything together for easy distribution.</span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap></span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Distributed File systems already exist, there is the Google file system [GFS] and the Gluster file system [GLS].</span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap></span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Redundant coordination already exists in some computer games. With multiple computers playing one network game, one computer is chosen as the server. If that server then goes offline the other computers re-elect a new server. This is similar to the coordination required for a distributed operating system.</span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap></span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Apache ZooKeeper also provides redundant distributed coordination software [AZK].</span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap></span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>We have distributed hash tables and distributed databases like Apache Cassandra [ACD], which work like a distributed shared memory. &nbsp;However their usually for persistent storage not like fast volatile shared memory required for task coordination.</span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap></span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Redundant distributable task contexts have been done well for embarrassingly parallel tasks, with map reduce and hadoop. Building on my proposed model should work well, but needs more work to show that its feasible, the context saving needs to be proven to not be too expensive, then it needs to be implemented.</span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap></span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>So how close. A year with a solid development team should be enough to put something together, using another distributed file system and data base. The big problem will be getting it popular enough to gain momentum. A lot of the companies that could provide the initial push of development seem to work in more of a embarrassingly parallel world, so wouldn't benefit from this.</span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap></span><br><h5 dir=ltr><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;font-weight:700;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>References</span></h5><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>[GMR] </span><span style=background-color:transparent;color:#000;font-family:Arial;font-size:8pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Jeffrey Dean & Sanjay Ghemawat (2004) &nbsp;MapReduce: Simplied Data Processing on Large Clusters. Google, Inc</span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>[AHD] </span><a href=http://hadoop.apache.org/><span style=background-color:transparent;color:#009;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:underline;vertical-align:baseline;white-space:pre-wrap>http://hadoop.apache.org</span></a><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap></span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>[GFS] </span><span style=background-color:transparent;color:#000;font-family:Arial;font-size:8pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>Sanjay Ghemawat, Howard Gobioff & Shun-Tak Leung (2003) The Google File System. Google, Inc</span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>[GLS] </span><a href=http://www.gluster.org/><span style=background-color:transparent;color:#009;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:underline;vertical-align:baseline;white-space:pre-wrap>http://www.gluster.org/</span></a><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap></span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>[AZK] </span><a href=http://zookeeper.apache.org/><span style=background-color:transparent;color:#009;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:underline;vertical-align:baseline;white-space:pre-wrap>http://zookeeper.apache.org/</span></a><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap></span><br><span style=background-color:transparent;color:#000;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap>[ACD] </span><a href=http://cassandra.apache.org/><span style=background-color:transparent;color:#009;font-family:Arial;font-size:11pt;font-style:normal;font-variant:normal;text-decoration:underline;vertical-align:baseline;white-space:pre-wrap>http://cassandra.apache.org/</span></a></div></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://stephenn.com/2012/01/concordion-acceptance-tests-from-scala/><span class=title>¬´ Prev</span><br><span>Concordion acceptance tests from Scala</span></a>
<a class=next href=https://stephenn.com/2011/07/server-updates-automate-it/><span class=title>Next ¬ª</span><br><span>Server updates: Automate it!</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share What should a Distributed Operating System should look like and how close are we to making one on twitter" href="https://twitter.com/intent/tweet/?text=What%20should%20a%20Distributed%20Operating%20System%20should%20look%20like%20and%20how%20close%20are%20we%20to%20making%20one&url=https%3a%2f%2fstephenn.com%2f2011%2f09%2fwhat-should-distributed-operating%2f&hashtags="><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share What should a Distributed Operating System should look like and how close are we to making one on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fstephenn.com%2f2011%2f09%2fwhat-should-distributed-operating%2f&title=What%20should%20a%20Distributed%20Operating%20System%20should%20look%20like%20and%20how%20close%20are%20we%20to%20making%20one&summary=What%20should%20a%20Distributed%20Operating%20System%20should%20look%20like%20and%20how%20close%20are%20we%20to%20making%20one&source=https%3a%2f%2fstephenn.com%2f2011%2f09%2fwhat-should-distributed-operating%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share What should a Distributed Operating System should look like and how close are we to making one on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fstephenn.com%2f2011%2f09%2fwhat-should-distributed-operating%2f&title=What%20should%20a%20Distributed%20Operating%20System%20should%20look%20like%20and%20how%20close%20are%20we%20to%20making%20one"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share What should a Distributed Operating System should look like and how close are we to making one on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fstephenn.com%2f2011%2f09%2fwhat-should-distributed-operating%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share What should a Distributed Operating System should look like and how close are we to making one on whatsapp" href="https://api.whatsapp.com/send?text=What%20should%20a%20Distributed%20Operating%20System%20should%20look%20like%20and%20how%20close%20are%20we%20to%20making%20one%20-%20https%3a%2f%2fstephenn.com%2f2011%2f09%2fwhat-should-distributed-operating%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share What should a Distributed Operating System should look like and how close are we to making one on telegram" href="https://telegram.me/share/url?text=What%20should%20a%20Distributed%20Operating%20System%20should%20look%20like%20and%20how%20close%20are%20we%20to%20making%20one&url=https%3a%2f%2fstephenn.com%2f2011%2f09%2fwhat-should-distributed-operating%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><footer class=footer><span>&copy; 2022 <a href=https://stephenn.com>Stephen's Tech Blog</a></span><div class=social-icons><a href=https://github.com/stephennancekivell target=_blank rel="noopener noreferrer me" title=Github><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a><a href=https://twitter.com/hi_stephen_n target=_blank rel="noopener noreferrer me" title=Twitter><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg></a><a href=https://www.linkedin.com/in/stephen-nancekivell-77003039 target=_blank rel="noopener noreferrer me" title=Linkedin><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg></a><a href=https://stackoverflow.com/users/893854/stephen target=_blank rel="noopener noreferrer me" title=Stackoverflow><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M2.913 16.041v6.848h17.599v-6.848M7.16 18.696h8.925M7.65 13.937l8.675 1.8M9.214 9.124l8.058 3.758M12.086 4.65l6.849 5.66M15.774 1.111l5.313 7.162"/></svg></a></div></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script></body></html>